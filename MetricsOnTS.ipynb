{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.utils import _safe_indexing\n",
    "from sklearn.metrics.pairwise import pairwise_distances_chunked\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_number_of_labels(n_labels, n_samples):\n",
    "    \"\"\"Check that number of labels are valid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_labels : int\n",
    "        Number of labels.\n",
    "\n",
    "    n_samples : int\n",
    "        Number of samples.\n",
    "    \"\"\"\n",
    "    if not 1 < n_labels < n_samples:\n",
    "        raise ValueError(\n",
    "            \"Number of labels is %d. Valid values are 2 to n_samples - 1 (inclusive)\"\n",
    "            % n_labels\n",
    "        )\n",
    "\n",
    "def davies_bouldin_score(X, labels,centroids):\n",
    "    \"\"\"Compute the Davies-Bouldin score.\n",
    "\n",
    "    The score is defined as the average similarity measure of each cluster with\n",
    "    its most similar cluster, where similarity is the ratio of within-cluster\n",
    "    distances to between-cluster distances. Thus, clusters which are farther\n",
    "    apart and less dispersed will result in a better score.\n",
    "\n",
    "    The minimum score is zero, with lower values indicating better clustering.\n",
    "\n",
    "    Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
    "\n",
    "    .. versionadded:: 0.20\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        A list of ``n_features``-dimensional data points. Each row corresponds\n",
    "        to a single data point.\n",
    "\n",
    "    labels : array-like of shape (n_samples,)\n",
    "        Predicted labels for each sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "        The resulting Davies-Bouldin score.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
    "       `\"A Cluster Separation Measure\"\n",
    "       <https://ieeexplore.ieee.org/document/4766909>`__.\n",
    "       IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
    "       PAMI-1 (2): 224-227\n",
    "    \"\"\"\n",
    "    X, labels = check_X_y(X, labels)\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "    n_samples, _ = X.shape\n",
    "    n_labels = len(le.classes_)\n",
    "    check_number_of_labels(n_labels, n_samples)\n",
    "\n",
    "    intra_dists = np.zeros(n_labels)\n",
    "    #centroids = np.zeros((n_labels, len(X[0])), dtype=float)\n",
    "    for k in range(n_labels):\n",
    "        cluster_k = _safe_indexing(X, labels == k)\n",
    "        #centroid = cluster_k.mean(axis=0)\n",
    "        #centroids[k] = centroid\n",
    "        intra_dists[k] = np.average(pairwise_distances(cluster_k, [centroids[k]]))\n",
    "\n",
    "    centroid_distances = pairwise_distances(centroids)\n",
    "\n",
    "    if np.allclose(intra_dists, 0) or np.allclose(centroid_distances, 0):\n",
    "        return 0.0\n",
    "\n",
    "    centroid_distances[centroid_distances == 0] = np.inf\n",
    "    combined_intra_dists = intra_dists[:, None] + intra_dists\n",
    "    scores = np.max(combined_intra_dists / centroid_distances, axis=1)\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/allPixelNDVIPoly.pickle', 'rb') as handle:\n",
    "    allPixelNDVIPoly3 = pickle.load(handle)\n",
    "\n",
    "with open('./pickles/newResa3.pickle', 'rb') as handle:\n",
    "    newResa3 = pickle.load(handle)\n",
    "\n",
    "with open('./pickles/allPixelNDVIPoly4.pickle', 'rb') as handle:\n",
    "    allPixelNDVIPoly4 = pickle.load(handle)\n",
    "\n",
    "with open('./pickles/newResa4.pickle', 'rb') as handle:\n",
    "    newResa4 = pickle.load(handle)\n",
    "\n",
    "with open('./pickles/allPixelNDVIPoly6.pickle', 'rb') as handle:\n",
    "    allPixelNDVIPoly6 = pickle.load(handle)\n",
    "\n",
    "with open('./pickles/newResa6.pickle', 'rb') as handle:\n",
    "    newResa6 = pickle.load(handle)\n",
    "\n",
    "num_cluster = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "allPixelNDVIPoly3 = allPixelNDVIPoly3[(newResa3<=11000) & (newResa3 >= 4000),:]\n",
    "newResa3 = newResa3[(newResa3<=11000) & (newResa3 >= 4000)]\n",
    "\n",
    "allPixelNDVIPoly4 = allPixelNDVIPoly4[(newResa4<=11000) & (newResa4 >= 4000),:]\n",
    "newResa4 = newResa4[(newResa4<=11000) & (newResa4 >= 4000)]\n",
    "print(len(newResa4))\n",
    "\n",
    "allPixelNDVIPoly6 = allPixelNDVIPoly6[(newResa6<=11000) & (newResa6 >= 4000),:]\n",
    "newResa6 = newResa6[(newResa6<=11000) & (newResa6 >= 4000)]\n",
    "print(len(newResa6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means su tutto l'asse temporale \n",
    "\n",
    "km_one3 = TimeSeriesKMeans(n_clusters=num_cluster, metric=\"euclidean\", max_iter=100,random_state=0)\n",
    "y_pred_one3 = km_one3.fit_predict(allPixelNDVIPoly3)\n",
    "\n",
    "km_one4 = TimeSeriesKMeans(n_clusters=num_cluster, metric=\"euclidean\", max_iter=100,random_state=0)\n",
    "y_pred_one4 = km_one4.fit_predict(allPixelNDVIPoly4)\n",
    "\n",
    "km_one6 = TimeSeriesKMeans(n_clusters=num_cluster, metric=\"euclidean\", max_iter=100,random_state=0)\n",
    "y_pred_one6 = km_one6.fit_predict(allPixelNDVIPoly6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "1.4875373563042251\n",
      "1.4875373563042251\n"
     ]
    }
   ],
   "source": [
    "print(len(allPixelNDVIPoly3[y_pred_one3 == 0][0]))\n",
    "print(len(km_one3.cluster_centers_[0].ravel()))\n",
    "print(np.linalg.norm(allPixelNDVIPoly3[y_pred_one3 == 0][0] - km_one3.cluster_centers_[0].ravel()))\n",
    "print(dist.euclidean(allPixelNDVIPoly3[y_pred_one3 == 0][0],km_one3.cluster_centers_[0].ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "(79950,) (533, 150)\n",
      "(150,)\n",
      "(140700,) (938, 150)\n"
     ]
    }
   ],
   "source": [
    "distancefromC = []\n",
    "centroids = []\n",
    "for cluster in range(0,num_cluster):\n",
    "    distancefromC.append([])\n",
    "    #\n",
    "    ClusterTs = allPixelNDVIPoly3[y_pred_one3 == cluster]\n",
    "    centroid = np.array(km_one3.cluster_centers_[cluster].ravel())\n",
    "    centroids = np.tile(centroid,(ClusterTs.shape[0]))\n",
    "    print(centroid.shape)\n",
    "    print(centroids.shape,ClusterTs.shape)\n",
    "    #for timeSeries in allPixelNDVIPoly3[y_pred_one3 == cluster]:\n",
    "    #    distancefromC[-1].append(dist.euclidean(timeSeries,km_one3.cluster_centers_[cluster].ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coesione del TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0\n",
      "nan\n",
      "nan\n",
      "cluster1\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_200\\468600622.py:3: RuntimeWarning: Mean of empty slice.\n",
      "  print(np.array(distancefromC[cluster]).mean())\n",
      "c:\\Users\\Leo\\.conda\\envs\\bigdata\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\Leo\\.conda\\envs\\bigdata\\lib\\site-packages\\numpy\\core\\_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Leo\\.conda\\envs\\bigdata\\lib\\site-packages\\numpy\\core\\_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "c:\\Users\\Leo\\.conda\\envs\\bigdata\\lib\\site-packages\\numpy\\core\\_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,num_cluster):\n",
    "    print(\"cluster\"+str(cluster))\n",
    "    print(np.array(distancefromC[cluster]).mean())\n",
    "    print(np.array(distancefromC[cluster]).std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daviesâ€“Bouldin index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indice dv del campo 3\n",
      "0.9182155745206122\n",
      "indice dv del campo 4\n",
      "0.6959551693972861\n",
      "indice dv del campo 6\n",
      "0.35611207971866476\n"
     ]
    }
   ],
   "source": [
    "#Rese 3 \n",
    "centroids = []\n",
    "for cluster in range(0,num_cluster):\n",
    "    centroids.append(km_one3.cluster_centers_[cluster].ravel())\n",
    "centroids = np.array(centroids)\n",
    "print(\"indice dv del campo\",3)\n",
    "print(davies_bouldin_score(allPixelNDVIPoly3,y_pred_one3,centroids))\n",
    "\n",
    "#Resa 4\n",
    "centroids = []\n",
    "for cluster in range(0,num_cluster):\n",
    "    centroids.append(km_one4.cluster_centers_[cluster].ravel())\n",
    "centroids = np.array(centroids)\n",
    "print(\"indice dv del campo\",4)\n",
    "print(davies_bouldin_score(allPixelNDVIPoly4,y_pred_one4,centroids))\n",
    "\n",
    "#Resa 6\n",
    "centroids = []\n",
    "for cluster in range(0,num_cluster):\n",
    "    centroids.append(km_one6.cluster_centers_[cluster].ravel())\n",
    "centroids = np.array(centroids)\n",
    "print(\"indice dv del campo\",6)\n",
    "print(davies_bouldin_score(allPixelNDVIPoly6,y_pred_one6,centroids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
